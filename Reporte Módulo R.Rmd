---
title: "Reporte Módulo Introducción a Lenguaje R" 
author: "Felipe Vivas"
date: "2021/12/20"
output: html_document
---

## Identificación de sentimientos generados por un evento deportivo a partir de los mensajes posteados en Twitter sobre el mismo

El objetivo del proyecto establecer si usando como base los tweets posteados durante un evento deportivo, podemos listar los sentimientos que se puedan identificar en los usuarios de la plataforma.

Para esto, utilizaremos la base de datos que encontramos en Kaggle de los ***Tweets durante el encuentro Real Madrid vs Liverpool 2018*** (Vivancos, X. V. (2018, May 26). Tweets during Real Madrid vs Liverpool. Kaggle. <https://www.kaggle.com/xvivancos/tweets-during-r-madrid-vs-liverpool-ucl-2018?select=TweetsChampions.json)>

### Librerias

En la generación del presente informe Usaremos las siguientes librerías:

```{r message=FALSE}
#librarys
library(DT)
library(readxl)
library(tidyverse)
library(ggplot2)
library(qdapRegex)
library(textclean)
library(tidytext)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(ggwordcloud)
library(treemapify)

```

```{r echo=FALSE}
#lectura de datos
# ![](https://www.poli.edu.co/sites/default/files/logos/logo-poli-politecnico-grancolombiano2018.png)
# {r, echo=FALSE, warning=FALSE,message=FALSE}
data_tweets = read_excel("Data/data_tweets.xlsx")
dtFields = read_excel("Data/data_fields.xlsx")
```

### Descripción de dimensiones y métricas de la base de datos **data_tweets**

La base de datos construida usando como base la fuente en Kaggel se ha estructurado de la siguiente manera:

```{r}
DT::datatable(dtFields, options = list(pageLength = 20))

```

### Principales incidencias en el levantamiento de la base de datos

Los principales inconvenientes al momento de leer el archivo fuente fueron:

- Formato JSON con registros separados por líneas en blanco.
  - Este modelo de almacenamiento de los registros en el archivo impide su lectura completa usando las librerías/ procesos normales. Se soluciona con los procesos implementados en Python para realizar lectura línea por línea. Cada línea es un registro JSON que se transforma e incorpora en el archivo de salida ***data_tweets***
- Dentro de la información disponible, los datos de _location_ de _lang_ corresponden a información que el usuario ingresa libremente, situación que obliga a aplicar una normalización de los datos disponibles así:
  - ***location***: La ubicación del usuario es un campo de respuesta abierta, por lo que para normalizar la información en una primera instancia se prepara un listado con los nombres de 249 países y se aplica un algoritmo que busque el nombre del país dentro del texto de _location_. Se actualiza la columna ***country*** con el nombre normalizado de la lista.
  - ***lang***: El idioma indicado por el usuario en la plataforma. Al ser un campo de respuesta abierta, existe gran cantidad de respuestas, para las cuales se aplicó la siguiente clasificación sobre la columna ***language***: 
    - ***english***: Todos los registros que tienen lang = 'en', 'en-AU', 'en-gb','en-IN'','en-US'
    - ***spanish***: Todos los registros que tienen lang = 'es', 'es-MX'
    - ***other***: Todos los demás registros
  - ***tweettext***: El texto del mensaje posteado en la plataforma. Sobre este campo vamos a correr los algoritmos de análisis de sentimientos. Encontramos elementos gramaticales que se convierten en texto "basura" como los https://, artículos, caracteres especiales, etc. Para este caso, generamos una columna ***textsentiment*** en dónde almacenaremos el resultado de la limpieza del campo en referencia.
  - ***métricas***: Dentro del los procesos que se están armando, una de las opciones es la de los análisis de métricas como _followers_, _tweets_, etc. Al tratar de analizar la información, por ejemplo con histogramas, diagramas de caja, gráficos de dispersión, vemos que los valores generan tendencias que desfiguran los análisis. Para solucionarlo, al momento de querer utilizar esta información, se van a aplicar rangos de datos a fin de tener una clasificación que permita este tipo de análisis.
  
Adicionalmente, la cantidad de observaciones en la data cruda, genera lentitud en los procesos de consolidación y análisis de la información.


### Tratamientos de datos
En la base de datos creamos las columnas _language, textsentiment, sentiment, country_ para clasificar y analizar la información. El idioma se ha clasificado como se indicó anteriormente así: ***english*** = en', 'en-AU', 'en-gb','en-IN'','en-US'; ***spanish*** = 'es', 'es-MX'. Igualmente ya se ha explicado la clasificación del país se ha clasificado a partir de un listado inicial de 249 opciones.
La limpieza del ***texttweet*** se reflejará en el campo ***textsentiment** aplicando los siguientes algoritmos:

Como primer paso, filtramos los tweets clasificados en ***english***
```{r}
# filtro de idioma reduce de 343,866 obs a 211,044
data_tweets_EN = data_tweets %>% select(screen_name, language, tweettext, textsentiment) %>% filter(language == "english") 
```
Sobre el conjunto de datos filtrado, aplicamos la limpieza del texto posteado:
```{r}
# limpiar url / tags
# limpiar emojis por palabra descriptiva
# limpiar <> de los emojis
# limpiar hastags
# limpiar menciones
# limpiar RT
# limpiar espacios
data_tweets_EN = data_tweets_EN %>% mutate(textsentiment = replace_url(tweettext, pattern = qdapRegex::grab("rm_url"), replacement = ""))
data_tweets_EN = data_tweets_EN %>% mutate(textsentiment = replace_emoji(textsentiment))
data_tweets_EN = data_tweets_EN %>% mutate(textsentiment = str_replace_all(textsentiment, "<\\w+>", ""))
data_tweets_EN = data_tweets_EN %>% mutate(textsentiment = str_replace_all(textsentiment, "\\d", ""))
data_tweets_EN = data_tweets_EN %>% mutate(textsentiment = str_replace_all(textsentiment, "#\\S+", ""))
data_tweets_EN = data_tweets_EN %>% mutate(textsentiment = str_replace_all(textsentiment, "@\\S+", ""))
data_tweets_EN = data_tweets_EN %>% mutate(textsentiment = str_replace(textsentiment, "RT ", ""))
data_tweets_EN = data_tweets_EN %>% mutate(textsentiment = trimws(textsentiment))
```

Como resultado, vemos que en los textos ya disponemos de datos más limpios, teniendo en cuenta que aún debemos profundizar en la limpieza de signos de puntuación, artículos gramaticales, etc.
```{r}
DT::datatable(head(data_tweets_EN))

```

Sobre esta nuevo set de datos, se trabajarán los mensajes únicos con el fin de extraer las palabras que serán usadas para análisis y clasificación de sentimientos. Se aplicará un filtro a mensajes de más de 10 caracteres para reducir la cantidad de observaciones.

```{r}
data_Unique = data_tweets_EN %>% filter(nchar(textsentiment) > 10) %>% distinct(textsentiment=tolower(textsentiment))
DT::datatable(head(data_Unique))
```

El siguiente paso aplicado corresponde a la limpieza de los _stop words_ o palabras que corresponden a adverbios, conjunciones, artículos, preposiciones.
```{r warning=FALSE}
data_Unique.corpus = data_Unique %>% tm::VectorSource() %>% tm::Corpus()

data_Unique.corpus.clean = data_Unique.corpus %>% 
  tm_map(removeWords, stopwords("english")) %>% # limpiar stopwords
  tm_map(removePunctuation) %>%   # limpiar signos de puntuación
  tm_map(stripWhitespace)   # eliminar espacios en blanco dobles

```

Con el texto limpio, construimos una tabla de frecuencias para las palabras disponibles. Generando un ranking tomamos las primeras 50 observaciones par en análisis de datos ya que tenemos un total 40,079 casos

```{r}
data_Unique.Frequency = data_Unique.corpus.clean %>% 
  tm::TermDocumentMatrix() %>% 
  as.matrix() %>% as.data.frame() %>% 
  tibble::rownames_to_column() %>%
  dplyr::rename(word = 1, freq = 2) %>%
  dplyr::arrange(desc(freq)) %>% top_n(50)

DT::datatable(head(data_Unique.Frequency))

```




### Análisis de datos 

Con la tabla generada durante el proceso de tratamiento de la información podemos analizar la calidad de los datos disponibles y entender si un proceso de _análisis de sentimientos_ puede generar resultados significativos para la hipótesis inicial

```{r warning=FALSE}
ggplot(data_Unique.Frequency, aes(area = freq, fill = word, label =  paste(word, freq, sep = "\n"))) +
  geom_treemap()+
  theme(legend.position = "none")+
  labs(title = "Frecuencia de palabras TOTAL TOP 50 (gráfico 1)")+
  scale_fill_brewer(palette = "Greens")+
  geom_treemap_text()

```

Se evidencia en este primer gráfico que la cantidad de textos que se relacionan con nombres propios o relacionados con descripciones propias del evento que claramente no son útiles para nuestro análisis pues no aportan ninguna evaluación de sentimientos.


Aplicando un filtrado de palabras relacionadas a jugadores, equipos, evento vemos:

```{r warning=FALSE}
filters = c("liverpool","karius","ronaldo","bale","salah","ramos","madrid","sergio","real","gareth","cristiano","champions","soccer","mane")
data_Unique.corpus.clean = data_Unique.corpus %>% 
  tm_map(removeWords, stopwords("english")) %>% # limpiar stopwords
  tm_map(removeWords, filters) %>% # limpiar textos particulares
  tm_map(removePunctuation) %>%   # limpiar signos de puntuación
  tm_map(stripWhitespace)   # eliminar espacios en blanco dobles

data_Unique.Frequency = data_Unique.corpus.clean %>% 
  tm::TermDocumentMatrix() %>% 
  as.matrix() %>% as.data.frame() %>% 
  tibble::rownames_to_column() %>%
  dplyr::rename(word = 1, freq = 2) %>%
  dplyr::arrange(desc(freq)) %>% top_n(50)


ggplot(data_Unique.Frequency, aes(area = freq, fill = word, label =  paste(word, freq, sep = "\n"))) +
  geom_treemap()+
  labs(title = "Frecuencia de palabras FILTRADO TOP 50 (gráfico 2)")+
  theme(legend.position = "none")+
  scale_fill_brewer(palette = "Blues")+
  geom_treemap_text(colour = "black")


data_Unique.Frequency = data_Unique.corpus.clean %>% 
  tm::TermDocumentMatrix() %>% 
  as.matrix() %>% as.data.frame() %>% 
  tibble::rownames_to_column() %>%
  dplyr::rename(word = 1, freq = 2) %>%
  dplyr::arrange(desc(freq)) %>% top_n(50)

ggplot(data_Unique.Frequency, aes(x=freq))+
  geom_histogram(color="grey", fill="lightblue")+
  labs(title = "Histograma de frecuencias TOP 50 (gráfico 3)")

```

Debido al origen y volumen de datos, se hace necesaria la reducción de observaciones mediante la selección de valores únicos de los tweets, adicional al tener una cota mínima de palabras dentro de cada posteo para descartar así datos que claramente van a afectar las tendencias.

En gráfico 1 podemos ver que dentro del top 50 existe una prevalencia de textos relacionados con jugadores, clubes de fútbol y otros términos relacionados con el evento. Igualmente vemos palabras de las que podemos deducir sentimientos como _heart, crying, eyes, best, smiling_. Estos textos pueden ser resultado del paso en el cuál se sustituyen emojis por palabras.

Al aplicar los filtros de nombres propios y de textos referentes al evento, vemos que la cantidad de palabras que pueden usarse para el análisis de sentimientos se incrementa. (gráfico 2)

Usando un histograma sobre las frecuencias de los primeras 50 palabras rankeadas (gráfico 3), vemos que el rango que debemos explorar es el que está por debajo de los 5,000


Filtramos entonces la información y analizamos los resultados

```{r}
data_Unique.Frequency = data_Unique.corpus.clean %>% 
  tm::TermDocumentMatrix() %>% 
  as.matrix() %>% as.data.frame() %>% 
  tibble::rownames_to_column() %>%
  dplyr::rename(word = 1, freq = 2) %>%
  dplyr::arrange(desc(freq)) 

data_Unique.Range = data_Unique.Frequency %>% filter(freq <= 5000) %>% arrange(desc(freq)) %>%  top_n(50) 

ggplot(data_Unique.Range, aes(area = freq, fill = freq, label =  paste(word, freq, sep = "\n"))) +
  geom_treemap()+
  labs(title = "Frecuencia de palabras Frecuencia < 5000 TOP 50 (gráfico 4)")+
  theme(legend.position = "none")+
  geom_treemap_text(colour = "white")

```

El gráfico obtenido con base en la data filtrada con frecuencias hasta 5,000 (gráfico 4) nos muestra como la mención de palabras es más homogénea y nos permite decidir si podemos seguir con el proceso de clasificación de sentimientos con este set de información. Igualmente se facilita la identificación de palabras que se deban filtrar en el paso de los textos específicos.

En este punto también podemos definir opciones alternas al análisis de sentimientos: analizar favoritismo en cuanto a jugadores, equipos. 

También al combinar otras métricas como followers, podemos inferir el impacto que los sentimientos evaluados puedan generar en los usuarios expuestos a las publicaciones.   

El siguiente paso es seleccionar las librerías y diccionarios que permitan una clasificación de sentimientos acertada, dependiendo claramente del nivel de detalle enfocando la evaluación en resultados que faciliten la confirmación de la hipótesis.


### Conclusiones
- Un eficiente resultado depende directamente de la calidad de la información recolectada.
- La normalización de variables se verá beneficiada al momento de determinar las dimensiones a analizar.
- Ante volúmenes altos de información, se debe tener en cuenta los tiempos de aplicación de normalizaciones así como el procesamiento de datos.
- La segmentación de información beneficia en manejo de los análisis de datos al permitir obtener resultados acotados y detallados.
- Las métricas obtenidas desde la plataforma y referentes a publicaciones sobre eventos deportivos, pueden ser una fuente alterna para establecer estrategías de publicidad digital teniendo en cuenta que los datos permiten la trazabilidad de las tendencias durante la duración de los mismos.
- Es claro que la limpieza de casos para esta base de datos debe tener un trabajo más profundo a fin de permitir información más limpia ya que como en los primeros resúmenes se aprecia, los términos que puedan aportar y ensuciar los resultados tienen bastante representación.



-----
Felipe Vivas
-----
